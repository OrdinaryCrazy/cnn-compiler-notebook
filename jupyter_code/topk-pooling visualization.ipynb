{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data     import DataLoader\n",
    "from torch_geometric.datasets import TUDataset, PPI\n",
    "from torch_geometric.nn       import TopKPooling, GCNConv, GINConv, SAGPooling\n",
    "import torch.nn.functional        as F\n",
    "import torch_geometric.transforms as T\n",
    "import torch\n",
    "from torch_geometric.nn      import global_max_pool\n",
    "from torch_scatter           import scatter_mean\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandleNodeAttention(object):\n",
    "    def __call__(self, data):\n",
    "        data.attn = torch.softmax(data.x, dim=0)\n",
    "        data.x = None\n",
    "        return data\n",
    "\n",
    "transform = T.Compose([HandleNodeAttention(), T.OneHotDegree(max_degree=14)])\n",
    "path = osp.join(osp.abspath(''), '..', 'data', 'TRIANGLES')\n",
    "dataset = TUDataset(path, name='TRIANGLES', use_node_attr=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset[:30000], batch_size=60, shuffle=True)\n",
    "\n",
    "# path = osp.join(osp.abspath(''), '..', 'data', 'PPI')\n",
    "# train_dataset = PPI(path, split='train')\n",
    "# val_dataset   = PPI(path, split='val')\n",
    "# test_dataset  = PPI(path, split='test')\n",
    "# train_loader  = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# val_loader    = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "# test_loader   = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = GCNConv(test_dataset.num_features, 64)\n",
    "#         self.topk_pool = TopKPooling(in_channels=64)\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x, edge_index, _, batch, perm, score = self.topk_pool(x, edge_index, None, batch)\n",
    "        \n",
    "#         return x, edge_index\n",
    "import networkx          as nx\n",
    "import matplotlib.pyplot as plt\n",
    "options = {\n",
    "    'node_color': 'black',\n",
    "    'node_size': 100,\n",
    "    'width': 3,\n",
    "}\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = GINConv(Seq(Lin(in_channels, 64), ReLU(), Lin(64, 64)))\n",
    "#         self.pool1s = SAGPooling(64, min_score=0.001, GNN=GCNConv)\n",
    "#         self.pool1t = TopKPooling(64,min_score=0.001)\n",
    "        self.pool1s = SAGPooling(64, ratio=0.5, GNN=GCNConv)\n",
    "        self.pool1t = TopKPooling(64,ratio=0.5)\n",
    "        self.conv2 = GINConv(Seq(Lin(64, 64), ReLU(), Lin(64, 64)))\n",
    "        self.pool2s = SAGPooling(64, ratio=0.5, GNN=GCNConv)\n",
    "        self.pool2t = TopKPooling(64, ratio=0.5)\n",
    "        self.conv3 = GINConv(Seq(Lin(64, 64), ReLU(), Lin(64, 64)))\n",
    "\n",
    "        self.lin = torch.nn.Linear(64, 1)\n",
    "        self.i = 0\n",
    "\n",
    "    def forward(self, data):\n",
    "        self.i += 1\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        origin_graph = nx.Graph()\n",
    "#         print(edge_index.cpu().numpy().tolist())\n",
    "        edges = edge_index.cpu().numpy().tolist()\n",
    "        origin_graph.add_edges_from(zip(edges[0],edges[1]))\n",
    "#         nx.draw_spectral(origin_graph, **options)\n",
    "#         plt.savefig(\"./origin_graphr\"+str(self.i)+\".png\")\n",
    "        nx.readwrite.graphml.write_graphml(origin_graph, \"./origin_graphr\"+str(self.i)+\".graphml\")\n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x_t, edge_index_t, _, batch_t, perm_t, score_t = self.pool1t(x, edge_index, None, batch)\n",
    "        x, edge_index, _, batch, perm, score = self.pool1s(x, edge_index, None, batch)\n",
    "        \n",
    "        topk1_graph = nx.Graph()\n",
    "        sag1__graph = nx.Graph()\n",
    "        edges = edge_index_t.cpu().numpy().tolist()\n",
    "        topk1_graph.add_edges_from(zip(edges[0],edges[1]))\n",
    "#         nx.draw_spectral(topk1_graph, **options)\n",
    "#         plt.savefig(\"./topk1_graph\"+str(self.i)+\".png\")\n",
    "        nx.readwrite.graphml.write_graphml(topk1_graph, \"./topk1_graph\"+str(self.i)+\".graphml\")\n",
    "        edges = edge_index.cpu().numpy().tolist()\n",
    "        sag1__graph.add_edges_from(zip(edges[0],edges[1]))\n",
    "        nx.draw_spectral(sag1__graph, **options)\n",
    "        plt.savefig(\"./sag1__graph\"+str(self.i)+\".png\")\n",
    "        nx.readwrite.graphml.write_graphml(sag1__graph, \"./sag1__graph\"+str(self.i)+\".graphml\")\n",
    "        \n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        \n",
    "        x_t, edge_index_t, _, batch_t, perm_t, score_t = self.pool2t(x, edge_index, None, batch)\n",
    "        x,   edge_index,   _, batch,   perm,   score   = self.pool2s(x, edge_index, None, batch)\n",
    "        topk2_graph = nx.Graph()\n",
    "        sag2__graph = nx.Graph()\n",
    "        edges = edge_index_t.cpu().numpy().tolist()\n",
    "        topk2_graph.add_edges_from(zip(edges[0],edges[1]))\n",
    "#         nx.draw_spectral(topk2_graph, **options)\n",
    "#         plt.savefig(\"./topk2_graph\"+str(self.i)+\".png\")\n",
    "        nx.readwrite.graphml.write_graphml(topk2_graph, \"./topk2_graph\"+str(self.i)+\".graphml\")\n",
    "        edges = edge_index.cpu().numpy().tolist()\n",
    "        sag2__graph.add_edges_from(zip(edges[0],edges[1]))\n",
    "#         nx.draw_spectral(sag2__graph, **options)\n",
    "#         plt.savefig(\"./sag2__graph\"+str(self.i)+\".png\")\n",
    "        nx.readwrite.graphml.write_graphml(sag2__graph, \"./sag2__graph\"+str(self.i)+\".graphml\")\n",
    "        \n",
    "        ratio = x.size(0) / data.x.size(0)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_max_pool(x, batch)\n",
    "        x = self.lin(x).view(-1)\n",
    "\n",
    "        attn_loss = F.kl_div(torch.log(score + 1e-14), data.attn[perm], reduction='none')\n",
    "        attn_loss = scatter_mean(attn_loss, batch)\n",
    "\n",
    "        return x, attn_loss, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(dataset.num_features).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import networkx as nx\n",
    "# model.train()\n",
    "# total_loss = 0\n",
    "# for data in train_loader:\n",
    "#     edge_index = data.edge_index\n",
    "    \n",
    "#     data = data.to(device)\n",
    "#     optimizer.zero_grad()\n",
    "#     out, attn_loss, _ = model(data)\n",
    "#     loss = ((out - data.y).pow(2) + 100 * attn_loss).mean()\n",
    "#     loss.backward()\n",
    "#     total_loss += loss.item() * data.num_graphs\n",
    "#     optimizer.step()\n",
    "#     print(total_loss / len(train_loader.dataset))\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, attn_loss, _ = model(data)\n",
    "        loss = ((out - data.y).pow(2) + 100 * attn_loss).mean()\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "        return\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "for epoch in range(1, 2):\n",
    "    loss = train(epoch)\n",
    "#     train_correct, train_ratio = test(train_loader)\n",
    "#     val_correct, val_ratio = test(val_loader)\n",
    "#     test_correct, test_ratio = test(test_loader)\n",
    "\n",
    "#     train_acc = train_correct.sum().item() / train_correct.size(0)\n",
    "#     val_acc = val_correct.sum().item() / val_correct.size(0)\n",
    "\n",
    "#     test_acc1 = test_correct[:5000].sum().item() / 5000\n",
    "#     test_acc2 = test_correct[5000:].sum().item() / 5000\n",
    "\n",
    "#     print(('Epoch: {:03d}, Loss: {:.4f}, Train: {:.3f}, Val: {:.3f}, '\n",
    "#            'Test Orig: {:.3f}, Test Large: {:.3f}, '\n",
    "#            'Train/Val/Test Ratio={:.3f}/{:.3f}/{:.3f}').format(\n",
    "#                epoch, loss, train_acc, val_acc, test_acc1, test_acc2,\n",
    "#                train_ratio, val_ratio, test_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([(1,2),(3,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a = torch.tensor([[7,8],[8,9]])\n",
    "# a = a.to(torch.device('cuda'))\n",
    "a = [[7,8],[8,9]]\n",
    "G = nx.Graph()\n",
    "# G.add_edges_from(a.numpy())\n",
    "G.add_edges_from(zip(a[0],a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = zip(a[0],a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-venv",
   "language": "python",
   "name": "gnn-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
